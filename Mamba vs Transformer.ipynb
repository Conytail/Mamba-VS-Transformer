{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================= Model 1 Summary (Mamba) =========================\n",
      "SimpleMambaSSM(\n",
      "  (input_proj): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (selective_gate): Linear(in_features=10, out_features=20, bias=True)\n",
      ")\n",
      "Total Parameters: 1060\n",
      "Trainable Parameters: 1060\n",
      "\n",
      "========================= Model 2 Summary (Transformer) =========================\n",
      "TransformerModel(\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=10, out_features=20, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=20, out_features=10, bias=True)\n",
      "        (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=10, out_features=20, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=20, out_features=10, bias=True)\n",
      "        (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "Total Parameters: 2831\n",
      "Trainable Parameters: 2831\n",
      "\n",
      "========================= Model Comparison =========================\n",
      "Difference in Total Parameters: 1771\n",
      "Difference in Trainable Parameters: 1771\n",
      "\n",
      "========================= Training Mamba Model =========================\n",
      "Epoch 1/5 | Training Loss: 12704.8078\n",
      "Epoch 2/5 | Training Loss: 8224.6599\n",
      "Epoch 3/5 | Training Loss: 5849.6648\n",
      "Epoch 4/5 | Training Loss: 4407.8232\n",
      "Epoch 5/5 | Training Loss: 3486.7942\n",
      "Total Training Time: 1.32 seconds\n",
      "\n",
      "\n",
      "========================= Training Transformer Model =========================\n",
      "Epoch 1/5 | Training Loss: 1.1871\n",
      "Epoch 2/5 | Training Loss: 1.0963\n",
      "Epoch 3/5 | Training Loss: 1.0233\n",
      "Epoch 4/5 | Training Loss: 1.0056\n",
      "Epoch 5/5 | Training Loss: 0.9971\n",
      "Total Training Time: 2.39 seconds\n",
      "\n",
      "\n",
      "========================= Training Time Comparison =========================\n",
      "Mamba Model Training Time: 1.32 seconds\n",
      "Transformer Model Training Time: 2.39 seconds\n",
      "Difference in Training Time: 1.07 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# Define a simplified Mamba model class\n",
    "class SimpleMambaSSM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleMambaSSM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.state_matrix = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.input_matrix = nn.Parameter(torch.randn(hidden_dim, input_dim))\n",
    "        self.output_matrix = nn.Parameter(torch.randn(hidden_dim, output_dim))\n",
    "        self.selective_gate = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, sequence_length, input_dim)\n",
    "        batch_size, sequence_length, _ = x.size()\n",
    "        hidden_state = torch.zeros(batch_size, self.hidden_dim).to(x.device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "            current_input = x[:, t, :]\n",
    "            # Compute selective gating mechanism\n",
    "            gate = torch.sigmoid(self.selective_gate(current_input))\n",
    "            \n",
    "            # Update hidden state with selective mechanism\n",
    "            hidden_state = gate * (torch.matmul(hidden_state, self.state_matrix) + torch.matmul(current_input, self.input_matrix.T))\n",
    "            \n",
    "            # Compute output\n",
    "            output = torch.matmul(hidden_state, self.output_matrix)\n",
    "            outputs.append(output.unsqueeze(1))\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, nhead, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, input_dim))  # Assuming max length of 100\n",
    "        encoder_layers = TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=hidden_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc_out = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, sequence_length, input_dim)\n",
    "        batch_size, sequence_length, _ = x.size()\n",
    "        x = x + self.positional_encoding[:, :sequence_length, :]\n",
    "        x = x.permute(1, 0, 2)  # Transformer expects (sequence_length, batch_size, input_dim)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)  # Back to (batch_size, sequence_length, input_dim)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "# Define a simple function to compare two models\n",
    "def compare_models(model1, model2):\n",
    "    # Print model summaries\n",
    "    print(\"\\n========================= Model 1 Summary (Mamba) =========================\")\n",
    "    print(model1)\n",
    "    total_params_model1 = sum(p.numel() for p in model1.parameters())\n",
    "    trainable_params_model1 = sum(p.numel() for p in model1.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params_model1}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params_model1}\")\n",
    "\n",
    "    print(\"\\n========================= Model 2 Summary (Transformer) =========================\")\n",
    "    print(model2)\n",
    "    total_params_model2 = sum(p.numel() for p in model2.parameters())\n",
    "    trainable_params_model2 = sum(p.numel() for p in model2.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params_model2}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params_model2}\")\n",
    "\n",
    "    # Print differences in parameter counts\n",
    "    print(\"\\n========================= Model Comparison =========================\")\n",
    "    print(f\"Difference in Total Parameters: {total_params_model2 - total_params_model1}\")\n",
    "    print(f\"Difference in Trainable Parameters: {trainable_params_model2 - trainable_params_model1}\")\n",
    "\n",
    "# Dummy models for comparison\n",
    "input_dim = 10\n",
    "hidden_dim = 20\n",
    "output_dim = 1\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "\n",
    "mamba_model = SimpleMambaSSM(input_dim, hidden_dim, output_dim)\n",
    "transformer_model = TransformerModel(input_dim, hidden_dim, output_dim, nhead=num_heads, num_layers=num_layers)\n",
    "\n",
    "# Compare models\n",
    "compare_models(mamba_model, transformer_model)\n",
    "\n",
    "# Dummy dataset for demonstration purposes\n",
    "class DummyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_samples, sequence_length, input_dim):\n",
    "        self.data = torch.randn(num_samples, sequence_length, input_dim)\n",
    "        self.targets = torch.randn(num_samples, sequence_length, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "# Define a training function with time measurement\n",
    "def timed_train(model, dataloader, criterion, optimizer, device, num_epochs=5):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Training Loss: {total_loss / len(dataloader):.4f}\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Total Training Time: {elapsed_time:.2f} seconds\\n\")\n",
    "    return elapsed_time\n",
    "\n",
    "# Example usage\n",
    "input_dim = 10\n",
    "hidden_dim = 20\n",
    "output_dim = 1\n",
    "num_samples = 100\n",
    "sequence_length = 5\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "\n",
    "dataset = DummyDataset(num_samples, sequence_length, input_dim)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Training Mamba model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mamba_model = mamba_model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(mamba_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\n========================= Training Mamba Model =========================\")\n",
    "mamba_training_time = timed_train(mamba_model, dataloader, criterion, optimizer, device)\n",
    "\n",
    "# Training Transformer model\n",
    "transformer_model = transformer_model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\n========================= Training Transformer Model =========================\")\n",
    "transformer_training_time = timed_train(transformer_model, dataloader, criterion, optimizer, device)\n",
    "\n",
    "# Comparison of training times\n",
    "print(\"\\n========================= Training Time Comparison =========================\")\n",
    "print(f\"Mamba Model Training Time: {mamba_training_time:.2f} seconds\")\n",
    "print(f\"Transformer Model Training Time: {transformer_training_time:.2f} seconds\")\n",
    "print(f\"Difference in Training Time: {transformer_training_time - mamba_training_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
